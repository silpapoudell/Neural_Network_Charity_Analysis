
Deliverable 1: Preprocessing the Data for a Neural Network
In [1]:
# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd 
application_df = pd.read_csv("charity_data.csv")
application_df.head()
Out[1]:
EIN	NAME	APPLICATION_TYPE	AFFILIATION	CLASSIFICATION	USE_CASE	ORGANIZATION	STATUS	INCOME_AMT	SPECIAL_CONSIDERATIONS	ASK_AMT	IS_SUCCESSFUL
0	10520599	BLUE KNIGHTS MOTORCYCLE CLUB	T10	Independent	C1000	ProductDev	Association	1	0	N	5000	1
1	10531628	AMERICAN CHESAPEAKE CLUB CHARITABLE TR	T3	Independent	C2000	Preservation	Co-operative	1	1-9999	N	108590	1
2	10547893	ST CLOUD PROFESSIONAL FIREFIGHTERS	T5	CompanySponsored	C3000	ProductDev	Association	1	0	N	5000	0
3	10553066	SOUTHSIDE ATHLETIC ASSOCIATION	T3	CompanySponsored	C2000	Preservation	Trust	1	10000-24999	N	6692	1
4	10556103	GENETIC RESEARCH INSTITUTE OF THE DESERT	T3	Independent	C1000	Heathcare	Trust	1	100000-499999	N	142590	1
In [2]:
# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df.drop(['EIN', 'NAME'], axis=1, inplace=True)
In [3]:
# Determine the number of unique values in each column.
application_df.nunique()
Out[3]:
APPLICATION_TYPE            17
AFFILIATION                  6
CLASSIFICATION              71
USE_CASE                     5
ORGANIZATION                 4
STATUS                       2
INCOME_AMT                   9
SPECIAL_CONSIDERATIONS       2
ASK_AMT                   8747
IS_SUCCESSFUL                2
dtype: int64
In [4]:
# Look at APPLICATION_TYPE value counts for binning
application_type_counts = application_df['APPLICATION_TYPE'].value_counts()
application_type_counts
Out[4]:
T3     27037
T4      1542
T6      1216
T5      1173
T19     1065
T8       737
T7       725
T10      528
T9       156
T13       66
T12       27
T2        16
T14        3
T25        3
T29        2
T15        2
T17        1
Name: APPLICATION_TYPE, dtype: int64
In [5]:
# Visualize the value counts of APPLICATION_TYPE
application_type_counts.plot.density()
Out[5]:
<matplotlib.axes._subplots.AxesSubplot at 0x1acfeaf32c8>

In [6]:
# Determine which values to replace if counts are less than ...?
replace_application = list(application_type_counts[application_type_counts < 500].index)

# Replace in dataframe
for app in replace_application:
    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,"Other")
    
# Check to make sure binning was successful
application_df.APPLICATION_TYPE.value_counts()
Out[6]:
T3       27037
T4        1542
T6        1216
T5        1173
T19       1065
T8         737
T7         725
T10        528
Other      276
Name: APPLICATION_TYPE, dtype: int64
In [7]:
# Look at CLASSIFICATION value counts for binning
classification_value_counts = application_df['CLASSIFICATION'].value_counts()
classification_value_counts
Out[7]:
C1000    17326
C2000     6074
C1200     4837
C3000     1918
C2100     1883
         ...  
C2150        1
C2500        1
C2600        1
C2561        1
C4500        1
Name: CLASSIFICATION, Length: 71, dtype: int64
In [8]:
# Visualize the value counts of CLASSIFICATION
classification_value_counts.plot.density()
Out[8]:
<matplotlib.axes._subplots.AxesSubplot at 0x1acff85f588>

In [9]:
# Determine which values to replace if counts are less than ..?
replace_class = list(classification_value_counts[classification_value_counts < 500].index)

# Replace in dataframe
for cls in replace_class:
    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,"Other")
    
# Check to make sure binning was successful
application_df.CLASSIFICATION.value_counts()
Out[9]:
C1000    17326
C2000     6074
C1200     4837
C3000     1918
C2100     1883
Other     1484
C7000      777
Name: CLASSIFICATION, dtype: int64
In [10]:
# Generate our categorical variable lists
application_cat = application_df.dtypes[application_df.dtypes == "object"].index.tolist()
application_cat
Out[10]:
['APPLICATION_TYPE',
 'AFFILIATION',
 'CLASSIFICATION',
 'USE_CASE',
 'ORGANIZATION',
 'INCOME_AMT',
 'SPECIAL_CONSIDERATIONS']
In [11]:
# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse=False)

# Fit and transform the OneHotEncoder using the categorical variable list
encode_df = pd.DataFrame(enc.fit_transform(application_df[application_cat]))

# Add the encoded variable names to the dataframe
encode_df.columns = enc.get_feature_names(application_cat)
encode_df.head()
Out[11]:
APPLICATION_TYPE_Other	APPLICATION_TYPE_T10	APPLICATION_TYPE_T19	APPLICATION_TYPE_T3	APPLICATION_TYPE_T4	APPLICATION_TYPE_T5	APPLICATION_TYPE_T6	APPLICATION_TYPE_T7	APPLICATION_TYPE_T8	AFFILIATION_CompanySponsored	...	INCOME_AMT_1-9999	INCOME_AMT_10000-24999	INCOME_AMT_100000-499999	INCOME_AMT_10M-50M	INCOME_AMT_1M-5M	INCOME_AMT_25000-99999	INCOME_AMT_50M+	INCOME_AMT_5M-10M	SPECIAL_CONSIDERATIONS_N	SPECIAL_CONSIDERATIONS_Y
0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
2	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	1.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
3	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	1.0	...	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
4	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
5 rows × 42 columns

In [12]:
# Merge one-hot encoded features and drop the originals
application_df = application_df.merge(encode_df, left_index=True,right_index=True)
application_df = application_df.drop(columns = application_cat)
application_df.head()
Out[12]:
STATUS	ASK_AMT	IS_SUCCESSFUL	APPLICATION_TYPE_Other	APPLICATION_TYPE_T10	APPLICATION_TYPE_T19	APPLICATION_TYPE_T3	APPLICATION_TYPE_T4	APPLICATION_TYPE_T5	APPLICATION_TYPE_T6	...	INCOME_AMT_1-9999	INCOME_AMT_10000-24999	INCOME_AMT_100000-499999	INCOME_AMT_10M-50M	INCOME_AMT_1M-5M	INCOME_AMT_25000-99999	INCOME_AMT_50M+	INCOME_AMT_5M-10M	SPECIAL_CONSIDERATIONS_N	SPECIAL_CONSIDERATIONS_Y
0	1	5000	1	0.0	1.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
1	1	108590	1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	...	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
2	1	5000	0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
3	1	6692	1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	...	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
4	1	142590	1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	...	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
5 rows × 45 columns

In [13]:
# Split our preprocessed data into our features and target arrays
y = application_df['IS_SUCCESSFUL'].values
X = application_df.drop(['IS_SUCCESSFUL'],1).values
# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
In [14]:
# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
Deliverable 2: Compile, Train and Evaluate the Model
In [15]:
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

number_input_features = len(X_train[0])
hidden_nodes_layer1 = 80
hidden_nodes_layer2 = 30

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu"))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 80)                3600      
_________________________________________________________________
dense_1 (Dense)              (None, 30)                2430      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 31        
=================================================================
Total params: 6,061
Trainable params: 6,061
Non-trainable params: 0
_________________________________________________________________
In [16]:
# Import checkpoint dependencies
import os
from tensorflow.keras.callbacks import ModelCheckpoint

# Define the checkpoint path and filenames
os.makedirs("checkpoints/",exist_ok=True)
checkpoint_path = "checkpoints/weights.{epoch:02d}.hdf5"
In [17]:
# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
In [18]:
# Create a callback that saves the model's weights every 5 epochs
cp_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=1000)
In [19]:
# Train the model
fit_model = nn.fit(X_train, y_train,epochs=30)
Epoch 1/30
804/804 [==============================] - 1s 759us/step - loss: 29901.4648 - accuracy: 0.4952
Epoch 2/30
804/804 [==============================] - 1s 756us/step - loss: 29615.9551 - accuracy: 0.4970
Epoch 3/30
804/804 [==============================] - 1s 766us/step - loss: 51711.4688 - accuracy: 0.4855
Epoch 4/30
804/804 [==============================] - 1s 761us/step - loss: 31568.7812 - accuracy: 0.5056
Epoch 5/30
804/804 [==============================] - 1s 765us/step - loss: 23869.0996 - accuracy: 0.4959
Epoch 6/30
804/804 [==============================] - 1s 784us/step - loss: 7318.3193 - accuracy: 0.5082
Epoch 7/30
804/804 [==============================] - 1s 802us/step - loss: 15235.2764 - accuracy: 0.5014
Epoch 8/30
804/804 [==============================] - 1s 763us/step - loss: 14995.5986 - accuracy: 0.5206
Epoch 9/30
804/804 [==============================] - 1s 820us/step - loss: 8467.9902 - accuracy: 0.4954
Epoch 10/30
804/804 [==============================] - 1s 834us/step - loss: 11444.8740 - accuracy: 0.5088
Epoch 11/30
804/804 [==============================] - 1s 851us/step - loss: 8477.3008 - accuracy: 0.5010
Epoch 12/30
804/804 [==============================] - 1s 862us/step - loss: 9229.2871 - accuracy: 0.5125
Epoch 13/30
804/804 [==============================] - 1s 791us/step - loss: 4890.6807 - accuracy: 0.5270
Epoch 14/30
804/804 [==============================] - 1s 791us/step - loss: 2671.6431 - accuracy: 0.5532
Epoch 15/30
804/804 [==============================] - 1s 791us/step - loss: 1836.9597 - accuracy: 0.5043
Epoch 16/30
804/804 [==============================] - 1s 794us/step - loss: 1074.3882 - accuracy: 0.5066
Epoch 17/30
804/804 [==============================] - 1s 833us/step - loss: 2274.5103 - accuracy: 0.5007
Epoch 18/30
804/804 [==============================] - 1s 808us/step - loss: 0.7683 - accuracy: 0.5321
Epoch 19/30
804/804 [==============================] - 1s 859us/step - loss: 0.6518 - accuracy: 0.5738
Epoch 20/30
804/804 [==============================] - 1s 866us/step - loss: 0.6300 - accuracy: 0.6563
Epoch 21/30
804/804 [==============================] - 1s 787us/step - loss: 0.6271 - accuracy: 0.6575
Epoch 22/30
804/804 [==============================] - 1s 788us/step - loss: 0.6253 - accuracy: 0.6596
Epoch 23/30
804/804 [==============================] - 1s 791us/step - loss: 0.6294 - accuracy: 0.6533
Epoch 24/30
804/804 [==============================] - 1s 793us/step - loss: 0.6934 - accuracy: 0.5178
Epoch 25/30
804/804 [==============================] - 1s 792us/step - loss: 0.6912 - accuracy: 0.5321
Epoch 26/30
804/804 [==============================] - 1s 803us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 27/30
804/804 [==============================] - 1s 787us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 28/30
804/804 [==============================] - 1s 790us/step - loss: 0.6912 - accuracy: 0.5321
Epoch 29/30
804/804 [==============================] - 1s 787us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 30/30
804/804 [==============================] - 1s 847us/step - loss: 0.6911 - accuracy: 0.5321
In [20]:
# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")
268/268 - 0s - loss: 0.8950 - accuracy: 0.6939
Loss: 0.8950213193893433, Accuracy: 0.6938775777816772
In [21]:
# Export our model to HDF5 file
nn.save("AlphabetSoupCharity.h5")
In [ ]:
