
Attempt 1: Removing Features
In [1]:
# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd 
application_df = pd.read_csv("charity_data.csv")
application_df.head()
Out[1]:
EIN	NAME	APPLICATION_TYPE	AFFILIATION	CLASSIFICATION	USE_CASE	ORGANIZATION	STATUS	INCOME_AMT	SPECIAL_CONSIDERATIONS	ASK_AMT	IS_SUCCESSFUL
0	10520599	BLUE KNIGHTS MOTORCYCLE CLUB	T10	Independent	C1000	ProductDev	Association	1	0	N	5000	1
1	10531628	AMERICAN CHESAPEAKE CLUB CHARITABLE TR	T3	Independent	C2000	Preservation	Co-operative	1	1-9999	N	108590	1
2	10547893	ST CLOUD PROFESSIONAL FIREFIGHTERS	T5	CompanySponsored	C3000	ProductDev	Association	1	0	N	5000	0
3	10553066	SOUTHSIDE ATHLETIC ASSOCIATION	T3	CompanySponsored	C2000	Preservation	Trust	1	10000-24999	N	6692	1
4	10556103	GENETIC RESEARCH INSTITUTE OF THE DESERT	T3	Independent	C1000	Heathcare	Trust	1	100000-499999	N	142590	1
In [2]:
# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df.drop(['EIN', 'NAME', 'USE_CASE'], axis=1, inplace=True)
In [3]:
# Determine the number of unique values in each column.
application_df.nunique()
Out[3]:
APPLICATION_TYPE            17
AFFILIATION                  6
CLASSIFICATION              71
ORGANIZATION                 4
STATUS                       2
INCOME_AMT                   9
SPECIAL_CONSIDERATIONS       2
ASK_AMT                   8747
IS_SUCCESSFUL                2
dtype: int64
In [4]:
# Look at APPLICATION_TYPE value counts for binning
application_type_counts = application_df['APPLICATION_TYPE'].value_counts()
application_type_counts
Out[4]:
T3     27037
T4      1542
T6      1216
T5      1173
T19     1065
T8       737
T7       725
T10      528
T9       156
T13       66
T12       27
T2        16
T14        3
T25        3
T29        2
T15        2
T17        1
Name: APPLICATION_TYPE, dtype: int64
In [5]:
# Visualize the value counts of APPLICATION_TYPE
application_type_counts.plot.density()
Out[5]:
<matplotlib.axes._subplots.AxesSubplot at 0x264a50373c8>

In [6]:
# Determine which values to replace if counts are less than ...?
replace_application = list(application_type_counts[application_type_counts < 500].index)

# Replace in dataframe
for app in replace_application:
    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,"Other")
    
# Check to make sure binning was successful
application_df.APPLICATION_TYPE.value_counts()
Out[6]:
T3       27037
T4        1542
T6        1216
T5        1173
T19       1065
T8         737
T7         725
T10        528
Other      276
Name: APPLICATION_TYPE, dtype: int64
In [7]:
# Look at CLASSIFICATION value counts for binning
classification_value_counts = application_df['CLASSIFICATION'].value_counts()
classification_value_counts
Out[7]:
C1000    17326
C2000     6074
C1200     4837
C3000     1918
C2100     1883
         ...  
C1236        1
C4200        1
C2600        1
C1820        1
C1580        1
Name: CLASSIFICATION, Length: 71, dtype: int64
In [8]:
# Visualize the value counts of CLASSIFICATION
classification_value_counts.plot.density()
Out[8]:
<matplotlib.axes._subplots.AxesSubplot at 0x264a5080d48>

In [9]:
# Determine which values to replace if counts are less than ..?
replace_class = list(classification_value_counts[classification_value_counts < 500].index)

# Replace in dataframe
for cls in replace_class:
    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,"Other")
    
# Check to make sure binning was successful
application_df.CLASSIFICATION.value_counts()
Out[9]:
C1000    17326
C2000     6074
C1200     4837
C3000     1918
C2100     1883
Other     1484
C7000      777
Name: CLASSIFICATION, dtype: int64
In [10]:
# Generate our categorical variable lists
application_cat = application_df.dtypes[application_df.dtypes == "object"].index.tolist()
application_cat
Out[10]:
['APPLICATION_TYPE',
 'AFFILIATION',
 'CLASSIFICATION',
 'ORGANIZATION',
 'INCOME_AMT',
 'SPECIAL_CONSIDERATIONS']
In [11]:
# Create a OneHotEncoder instance
enc = OneHotEncoder(sparse=False)

# Fit and transform the OneHotEncoder using the categorical variable list
encode_df = pd.DataFrame(enc.fit_transform(application_df[application_cat]))

# Add the encoded variable names to the dataframe
encode_df.columns = enc.get_feature_names(application_cat)
encode_df.head()
Out[11]:
APPLICATION_TYPE_Other	APPLICATION_TYPE_T10	APPLICATION_TYPE_T19	APPLICATION_TYPE_T3	APPLICATION_TYPE_T4	APPLICATION_TYPE_T5	APPLICATION_TYPE_T6	APPLICATION_TYPE_T7	APPLICATION_TYPE_T8	AFFILIATION_CompanySponsored	...	INCOME_AMT_1-9999	INCOME_AMT_10000-24999	INCOME_AMT_100000-499999	INCOME_AMT_10M-50M	INCOME_AMT_1M-5M	INCOME_AMT_25000-99999	INCOME_AMT_50M+	INCOME_AMT_5M-10M	SPECIAL_CONSIDERATIONS_N	SPECIAL_CONSIDERATIONS_Y
0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	...	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
2	0.0	0.0	0.0	0.0	0.0	1.0	0.0	0.0	0.0	1.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
3	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	1.0	...	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
4	0.0	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
5 rows × 37 columns

In [12]:
# Merge one-hot encoded features and drop the originals
application_df = application_df.merge(encode_df, left_index=True,right_index=True)
application_df = application_df.drop(columns = application_cat)
application_df.head()
Out[12]:
STATUS	ASK_AMT	IS_SUCCESSFUL	APPLICATION_TYPE_Other	APPLICATION_TYPE_T10	APPLICATION_TYPE_T19	APPLICATION_TYPE_T3	APPLICATION_TYPE_T4	APPLICATION_TYPE_T5	APPLICATION_TYPE_T6	...	INCOME_AMT_1-9999	INCOME_AMT_10000-24999	INCOME_AMT_100000-499999	INCOME_AMT_10M-50M	INCOME_AMT_1M-5M	INCOME_AMT_25000-99999	INCOME_AMT_50M+	INCOME_AMT_5M-10M	SPECIAL_CONSIDERATIONS_N	SPECIAL_CONSIDERATIONS_Y
0	1	5000	1	0.0	1.0	0.0	0.0	0.0	0.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
1	1	108590	1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	...	1.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
2	1	5000	0	0.0	0.0	0.0	0.0	0.0	1.0	0.0	...	0.0	0.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
3	1	6692	1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	...	0.0	1.0	0.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
4	1	142590	1	0.0	0.0	0.0	1.0	0.0	0.0	0.0	...	0.0	0.0	1.0	0.0	0.0	0.0	0.0	0.0	1.0	0.0
5 rows × 40 columns

In [13]:
# Split our preprocessed data into our features and target arrays
y = application_df['IS_SUCCESSFUL'].values
X = application_df.drop(['IS_SUCCESSFUL'],1).values
# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)
In [14]:
# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
In [15]:
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

number_input_features = len(X_train[0])
hidden_nodes_layer1 = 80
hidden_nodes_layer2 = 30

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu"))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 80)                3200      
_________________________________________________________________
dense_1 (Dense)              (None, 30)                2430      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 31        
=================================================================
Total params: 5,661
Trainable params: 5,661
Non-trainable params: 0
_________________________________________________________________
In [16]:
# Import checkpoint dependencies
import os
from tensorflow.keras.callbacks import ModelCheckpoint

# Define the checkpoint path and filenames
os.makedirs("checkpoints/",exist_ok=True)
checkpoint_path = "checkpoints/weights.{epoch:02d}.hdf5"
In [17]:
# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
In [18]:
# Create a callback that saves the model's weights every 5 epochs
cp_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=1000)
In [19]:
# Train the model
fit_model = nn.fit(X_train, y_train,epochs=30)
Epoch 1/30
804/804 [==============================] - 1s 762us/step - loss: 77079.1406 - accuracy: 0.4888
Epoch 2/30
804/804 [==============================] - 1s 760us/step - loss: 77675.7891 - accuracy: 0.5132
Epoch 3/30
804/804 [==============================] - 1s 767us/step - loss: 0.6923 - accuracy: 0.5321
Epoch 4/30
804/804 [==============================] - 1s 757us/step - loss: 0.6914 - accuracy: 0.5321
Epoch 5/30
804/804 [==============================] - 1s 758us/step - loss: 0.6912 - accuracy: 0.5321
Epoch 6/30
804/804 [==============================] - 1s 774us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 7/30
804/804 [==============================] - 1s 750us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 8/30
804/804 [==============================] - 1s 754us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 9/30
804/804 [==============================] - 1s 766us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 10/30
804/804 [==============================] - 1s 785us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 11/30
804/804 [==============================] - 1s 980us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 12/30
804/804 [==============================] - 1s 754us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 13/30
804/804 [==============================] - 1s 880us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 14/30
804/804 [==============================] - 1s 911us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 15/30
804/804 [==============================] - 1s 812us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 16/30
804/804 [==============================] - 1s 954us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 17/30
804/804 [==============================] - 1s 815us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 18/30
804/804 [==============================] - 1s 820us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 19/30
804/804 [==============================] - 1s 766us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 20/30
804/804 [==============================] - 1s 768us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 21/30
804/804 [==============================] - 1s 777us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 22/30
804/804 [==============================] - 1s 772us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 23/30
804/804 [==============================] - 1s 768us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 24/30
804/804 [==============================] - 1s 772us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 25/30
804/804 [==============================] - 1s 812us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 26/30
804/804 [==============================] - 1s 818us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 27/30
804/804 [==============================] - 1s 775us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 28/30
804/804 [==============================] - 1s 794us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 29/30
804/804 [==============================] - 1s 858us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 30/30
804/804 [==============================] - 1s 799us/step - loss: 0.6911 - accuracy: 0.5321
In [20]:
# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")
268/268 - 0s - loss: 0.6621 - accuracy: 0.6302
Loss: 0.662061870098114, Accuracy: 0.6302040815353394
In [21]:
# Export our model to HDF5 file
nn.save("AlphabetSoupCharity_Attempt1.h5")
Attempt 2: Adding Additional neurons to hidden layers and additional layers are added
In [22]:
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

number_input_features = len(X_train[0])
hidden_nodes_layer1 = 100
hidden_nodes_layer2 = 50
hidden_nodes_layer3 = 20

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu"))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Third hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation="relu"))


# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model
nn.summary()
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_3 (Dense)              (None, 100)               4000      
_________________________________________________________________
dense_4 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_5 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 21        
=================================================================
Total params: 10,091
Trainable params: 10,091
Non-trainable params: 0
_________________________________________________________________
In [23]:
# Import checkpoint dependencies
import os
from tensorflow.keras.callbacks import ModelCheckpoint

# Define the checkpoint path and filenames
os.makedirs("checkpoints/",exist_ok=True)
checkpoint_path = "checkpoints/weights.{epoch:02d}.hdf5"
In [24]:
# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
In [25]:
# Create a callback that saves the model's weights every 5 epochs
cp_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=1000)
In [26]:
# Train the model
fit_model = nn.fit(X_train, y_train,epochs=30)
Epoch 1/30
804/804 [==============================] - 1s 856us/step - loss: 20664.4434 - accuracy: 0.4885
Epoch 2/30
804/804 [==============================] - 1s 868us/step - loss: 761.5153 - accuracy: 0.5239
Epoch 3/30
804/804 [==============================] - 1s 847us/step - loss: 0.6917 - accuracy: 0.5321
Epoch 4/30
804/804 [==============================] - 1s 847us/step - loss: 0.6912 - accuracy: 0.5321
Epoch 5/30
804/804 [==============================] - 1s 882us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 6/30
804/804 [==============================] - 1s 875us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 7/30
804/804 [==============================] - 1s 861us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 8/30
804/804 [==============================] - 1s 863us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 9/30
804/804 [==============================] - 1s 857us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 10/30
804/804 [==============================] - 1s 921us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 11/30
804/804 [==============================] - 1s 975us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 12/30
804/804 [==============================] - 1s 946us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 13/30
804/804 [==============================] - 1s 959us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 14/30
804/804 [==============================] - 1s 822us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 15/30
804/804 [==============================] - 1s 934us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 16/30
804/804 [==============================] - 1s 840us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 17/30
804/804 [==============================] - 1s 939us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 18/30
804/804 [==============================] - 1s 845us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 19/30
804/804 [==============================] - 1s 841us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 20/30
804/804 [==============================] - 1s 833us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 21/30
804/804 [==============================] - 1s 835us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 22/30
804/804 [==============================] - 1s 820us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 23/30
804/804 [==============================] - 1s 826us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 24/30
804/804 [==============================] - 1s 840us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 25/30
804/804 [==============================] - 1s 833us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 26/30
804/804 [==============================] - 1s 840us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 27/30
804/804 [==============================] - 1s 832us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 28/30
804/804 [==============================] - 1s 835us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 29/30
804/804 [==============================] - 1s 837us/step - loss: 0.6911 - accuracy: 0.5321
Epoch 30/30
804/804 [==============================] - 1s 830us/step - loss: 0.6911 - accuracy: 0.5321
In [27]:
# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")
268/268 - 0s - loss: 0.6980 - accuracy: 0.5348
Loss: 0.6979904174804688, Accuracy: 0.534810483455658
In [28]:
# Export our model to HDF5 file
nn.save("AlphabetSoupCharity_Attempt2.h5")
Attempt 3: Changing activation function of output layer
In [29]:
# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

number_input_features = len(X_train[0])
hidden_nodes_layer1 = 100
hidden_nodes_layer2 = 50
hidden_nodes_layer3 = 20

nn = tf.keras.models.Sequential()

# First hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu"))

# Second hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Third hidden layer
nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation="relu"))


# Output layer
nn.add(tf.keras.layers.Dense(units=1, activation="tanh"))

# Check the structure of the model
nn.summary()
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_7 (Dense)              (None, 100)               4000      
_________________________________________________________________
dense_8 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_9 (Dense)              (None, 20)                1020      
_________________________________________________________________
dense_10 (Dense)             (None, 1)                 21        
=================================================================
Total params: 10,091
Trainable params: 10,091
Non-trainable params: 0
_________________________________________________________________
In [30]:
# Import checkpoint dependencies
import os
from tensorflow.keras.callbacks import ModelCheckpoint

# Define the checkpoint path and filenames
os.makedirs("checkpoints/",exist_ok=True)
checkpoint_path = "checkpoints/weights.{epoch:02d}.hdf5"
In [31]:
# Compile the model
nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
In [32]:
# Create a callback that saves the model's weights every 5 epochs
cp_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=1000)
In [33]:
# Train the model
fit_model = nn.fit(X_train, y_train,epochs=30)
Epoch 1/30
804/804 [==============================] - 1s 856us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 2/30
804/804 [==============================] - 1s 869us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 3/30
804/804 [==============================] - 1s 861us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 4/30
804/804 [==============================] - 1s 865us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 5/30
804/804 [==============================] - 1s 867us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 6/30
804/804 [==============================] - 1s 864us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 7/30
804/804 [==============================] - 1s 860us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 8/30
804/804 [==============================] - 1s 861us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 9/30
804/804 [==============================] - 1s 856us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 10/30
804/804 [==============================] - 1s 914us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 11/30
804/804 [==============================] - 1s 901us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 12/30
804/804 [==============================] - 1s 847us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 13/30
804/804 [==============================] - 1s 826us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 14/30
804/804 [==============================] - 1s 846us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 15/30
804/804 [==============================] - 1s 866us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 16/30
804/804 [==============================] - 1s 865us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 17/30
804/804 [==============================] - 1s 867us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 18/30
804/804 [==============================] - 1s 860us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 19/30
804/804 [==============================] - 1s 860us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 20/30
804/804 [==============================] - 1s 869us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 21/30
804/804 [==============================] - 1s 863us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 22/30
804/804 [==============================] - 1s 860us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 23/30
804/804 [==============================] - 1s 867us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 24/30
804/804 [==============================] - 1s 861us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 25/30
804/804 [==============================] - 1s 860us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 26/30
804/804 [==============================] - 1s 863us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 27/30
804/804 [==============================] - 1s 859us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 28/30
804/804 [==============================] - 1s 854us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 29/30
804/804 [==============================] - 1s 842us/step - loss: 7.1343 - accuracy: 0.5321
Epoch 30/30
804/804 [==============================] - 1s 830us/step - loss: 7.1343 - accuracy: 0.5321
In [34]:
# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")
268/268 - 0s - loss: 1.7025 - accuracy: 0.5041
Loss: 1.702485203742981, Accuracy: 0.5041399598121643
In [35]:
# Export our model to HDF5 file
nn.save("AlphabetSoupCharity_Optimization.h5")
In [ ]:
